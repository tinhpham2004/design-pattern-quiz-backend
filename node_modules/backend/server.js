const express = require("express");
const { GoogleGenerativeAI } = require("@google/generative-ai");
const dotenv = require("dotenv");
const cors = require("cors");

dotenv.config();

const app = express();
const port = 5000;
app.use(express.json());
app.use(cors());

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);

// Using the generation configuration explicitly to address API requirements
const generationConfig = {
  temperature: 0.7,
  topK: 1,
  topP: 1,
  maxOutputTokens: 1024, // Reducing token count to avoid quota issues
};

// For @google/generative-ai version 0.24.1, using gemini-1.5-flash as suggested
const model = genAI.getGenerativeModel({ 
  model: "gemini-1.5-flash",
  generationConfig
});

app.post("/api/get-recommendation", async (req, res) => {
  const { prompt } = req.body;
  console.log("Received prompt:", prompt);
  
  try {
    console.log("Calling Gemini API with model:", model.modelName);
    const result = await model.generateContent(prompt);
    console.log("API response received");
    
    // Based on our tests, we know that result.response.text is a function
    // and is the most reliable way to get the response text
    const responseText = result.response.text();
    console.log("Response text:", responseText);
    res.json({ recommendation: responseText });
  } catch (error) {
    console.error("Lỗi khi gọi Gemini API:", error);
    console.error("Error details:", error.message);
    
    res.status(500).json({ 
      error: "Lỗi khi tạo đề xuất từ AI.", 
      details: error.message,
      status: error.status || 'unknown'
    });
  }
});

app.listen(port, () => {
  console.log(`Server đang chạy trên port ${port}`);
});
